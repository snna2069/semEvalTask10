{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13712259,"sourceType":"datasetVersion","datasetId":8723334}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code]\n# ENV BOOTSTRAP: protobuf crash guard, optional installs on Kaggle, version-proof TrainingArguments\nimport os, sys, socket, importlib, subprocess, inspect\nos.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n\ndef internet_ok(host=\"pypi.org\"):\n    try:\n        socket.gethostbyname(host)\n        return True\n    except Exception:\n        return False\n\nif internet_ok():\n    subprocess.run(\n        [\n            sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-U\",\n            \"protobuf<5\", \"pyarrow>=14,<20\",\n            \"transformers==4.44.2\", \"datasets==2.19.0\",\n            \"accelerate==1.0.1\", \"sentencepiece==0.2.0\",\n            \"scikit-learn==1.5.2\"\n        ],\n        check=False,\n    )\n\ndef _imp(name):\n    try:\n        return importlib.import_module(name)\n    except Exception as e:\n        print(f\"import {name} failed: {e}\")\n        return None\n\ntfm = _imp(\"transformers\")\ndsets = _imp(\"datasets\")\ntorch = _imp(\"torch\")\nspm = _imp(\"sentencepiece\")\n\n# Patch TrainingArguments to drop unknown keys on older transformers versions\ntry:\n    from transformers import TrainingArguments as _TA\n    sig = inspect.signature(_TA.__init__)\n    need_patch = any(k not in sig.parameters for k in [\n        \"evaluation_strategy\", \"eval_strategy\", \"save_strategy\",\n        \"gradient_checkpointing\", \"save_safetensors\", \"lr_scheduler_type\"\n    ])\n    if need_patch:\n        _Base = _TA\n        class _PatchedTrainingArguments(_Base):  # type: ignore\n            def __init__(self, *args, **kwargs):\n                allowed = set(inspect.signature(_Base.__init__).parameters.keys())\n                filtered = {k: v for k, v in kwargs.items() if k in allowed}\n                dropped = sorted(set(kwargs) - set(filtered))\n                if dropped:\n                    print(\"TrainingArguments dropping unsupported keys:\", dropped)\n                super().__init__(*args, **filtered)\n        import transformers as _m\n        _m.TrainingArguments = _PatchedTrainingArguments\n        print(\"TrainingArguments patched for compatibility\")\nexcept Exception as e:\n    print(\"TrainingArguments patch skipped:\", e)\n\nif tfm: print(\"transformers:\", tfm.__version__)\nif dsets: print(\"datasets:\", dsets.__version__)\nif torch:\n    print(\"torch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available())\n    if torch.cuda.is_available():\n        print(\"GPU:\", torch.cuda.get_device_name(0))\nprint(\"Environment ready.\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code]\nimport os, json, random, numpy as np, pandas as pd, torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom datasets import Dataset, DatasetDict\nfrom transformers import (\n    AutoTokenizer, AutoModelForSequenceClassification,\n    DataCollatorWithPadding, TrainingArguments, Trainer, set_seed\n)\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); set_seed(SEED)\n\nMODEL_NAME = \"google/rembert\"   # RemBERT uses SentencePiece tokenizer\nMAX_LEN    = 160                # accuracy/VRAM tradeoff\n\nCANDIDATE_PATHS = [\n    \"/kaggle/input/model4-dataset/train_data.json\",\n]\nDATA_PATH = next((p for p in CANDIDATE_PATHS if os.path.exists(p)), None)\nassert DATA_PATH, f\"Place train_data.json in one of: {CANDIDATE_PATHS}\"\nprint(\"Data:\", DATA_PATH)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T05:47:03.712042Z","iopub.execute_input":"2025-11-13T05:47:03.712319Z","iopub.status.idle":"2025-11-13T05:47:22.407328Z","shell.execute_reply.started":"2025-11-13T05:47:03.712299Z","shell.execute_reply":"2025-11-13T05:47:22.406612Z"}},"outputs":[{"name":"stderr","text":"2025-11-13 05:47:08.288599: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763012828.468738     118 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763012828.518201     118 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Data: /kaggle/input/model4-dataset/train_data.json\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# %% [code]\nwith open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\n\ndf = pd.DataFrame(data)\nassert {\"syllogism\", \"validity\"}.issubset(df.columns)\ndf = df[[\"syllogism\", \"validity\"]].dropna().rename(columns={\"syllogism\": \"text\", \"validity\": \"label\"})\ndf[\"label\"] = df[\"label\"].astype(int)\n\nprint(df.head(3))\nprint(\"Label distribution:\", df[\"label\"].value_counts(normalize=True).round(3).to_dict())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T05:47:33.107569Z","iopub.execute_input":"2025-11-13T05:47:33.108498Z","iopub.status.idle":"2025-11-13T05:47:33.148620Z","shell.execute_reply.started":"2025-11-13T05:47:33.108468Z","shell.execute_reply":"2025-11-13T05:47:33.147907Z"}},"outputs":[{"name":"stdout","text":"                                                text  label\n0  All cars are a type of vehicle. No animal is a...      0\n1  Nothing that is a soda is a juice. A portion o...      1\n2  Everything that is a planet is a celestial bod...      0\nLabel distribution: {0: 0.5, 1: 0.5}\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# %% [code]\ntrain_df, val_df = train_test_split(\n    df, test_size=0.15, random_state=SEED, stratify=df[\"label\"]\n)\n\nds = DatasetDict({\n    \"train\": Dataset.from_pandas(train_df.reset_index(drop=True)),\n    \"validation\": Dataset.from_pandas(val_df.reset_index(drop=True)),\n})\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n\ndef tok(batch):\n    return tokenizer(batch[\"text\"], truncation=True, max_length=MAX_LEN)\n\ncols_to_remove = [c for c in ds[\"train\"].column_names if c not in [\"text\", \"label\"]]\nds_tok = ds.map(tok, batched=True, remove_columns=cols_to_remove)\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\nprint(ds_tok)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T05:47:36.674267Z","iopub.execute_input":"2025-11-13T05:47:36.675059Z","iopub.status.idle":"2025-11-13T05:47:40.046443Z","shell.execute_reply.started":"2025-11-13T05:47:36.675032Z","shell.execute_reply":"2025-11-13T05:47:40.045613Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/263 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd0749ee3c38451bb702fd2a1cf4fdb6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.model:   0%|          | 0.00/4.70M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"607d74d19e194ccd9037469d45d20bd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92b638520f1f4cfeaf720a1d8113034c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/156 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dac1a9bba7d742229d4f87ff77ff37dc"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/816 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5dfb8d77b4e4a64915da10c0ca63cea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/144 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca96f771914042d48bbadc2eb4dc03e9"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 816\n    })\n    validation: Dataset({\n        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 144\n    })\n})\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# %% [code]\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=1)\n    acc = accuracy_score(labels, preds)\n    prec, rec, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\", zero_division=0)\n    return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T05:47:42.788294Z","iopub.execute_input":"2025-11-13T05:47:42.789000Z","iopub.status.idle":"2025-11-13T05:47:42.792972Z","shell.execute_reply.started":"2025-11-13T05:47:42.788976Z","shell.execute_reply":"2025-11-13T05:47:42.792171Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# %% [code]\nfrom collections import Counter\n\nnum_labels = 2\ncounts = Counter(ds[\"train\"][\"label\"])\nfreqs = np.array([counts.get(i, 0) for i in range(num_labels)], dtype=np.float32)\ninv = 1.0 / np.clip(freqs, 1.0, None)\nclass_weights = torch.tensor(inv / inv.sum() * num_labels, dtype=torch.float32)\n\nprint(\"Class counts:\", dict(counts))\nprint(\"Class weights:\", class_weights.tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T05:47:44.998658Z","iopub.execute_input":"2025-11-13T05:47:44.999234Z","iopub.status.idle":"2025-11-13T05:47:45.019324Z","shell.execute_reply.started":"2025-11-13T05:47:44.999209Z","shell.execute_reply":"2025-11-13T05:47:45.018331Z"}},"outputs":[{"name":"stdout","text":"Class counts: {0: 408, 1: 408}\nClass weights: [1.0, 1.0]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# %% [code]\n# Fine-tuning for accuracy: unfreeze last 3 blocks + pooler + classifier\n# Layer-wise learning rates (higher on head), cosine schedule with warmup,\n# class-weighted loss, small per-device batch with gradient accumulation.\nimport gc, inspect\nfrom torch.optim import AdamW\nfrom transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification\nfrom transformers.trainer_callback import TrainerCallback\n\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nOUT_DIR = \"outputs_rembert\"\n\n# Load model\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n\n# Memory guards\nif hasattr(model, \"config\") and hasattr(model.config, \"use_cache\"):\n    model.config.use_cache = False\ntry:\n    if hasattr(model, \"gradient_checkpointing_enable\"):\n        sig = inspect.signature(model.gradient_checkpointing_enable)\n        if \"use_reentrant\" in sig.parameters:\n            model.gradient_checkpointing_enable(use_reentrant=False)\n        else:\n            model.gradient_checkpointing_enable()\n    elif hasattr(model, \"enable_input_require_grads\"):\n        model.enable_input_require_grads()\n        if hasattr(model, \"config\"):\n            model.config.gradient_checkpointing = True\nexcept Exception:\n    if hasattr(model, \"config\"):\n        model.config.gradient_checkpointing = True\n\n# Unfreeze strategy\nN_TRAIN_LAYERS = 3\nfor p in model.rembert.parameters():\n    p.requires_grad = False\nfor p in model.classifier.parameters():\n    p.requires_grad = True\nif hasattr(model.rembert, \"pooler\"):\n    for p in model.rembert.pooler.parameters():\n        p.requires_grad = True\nenc_layers = getattr(model.rembert.encoder, \"layer\", None)\nif enc_layers is not None and N_TRAIN_LAYERS > 0:\n    for blk in enc_layers[-N_TRAIN_LAYERS:]:\n        for p in blk.parameters():\n            p.requires_grad = True\n\ntrainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal = sum(p.numel() for p in model.parameters())\nprint(f\"Trainable params: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)\")\n\n# Hyperparameters\nEPOCHS = 5\nBSZ = 2\nACCUM = 16\nLR_HEAD = 4e-5\nLR_LAST = 2e-5\nWD = 0.01\nLOG_STEPS = 50\n\n# TrainingArguments (version-proof eval key)\nsig_ta = inspect.signature(TrainingArguments.__init__)\neval_key = \"eval_strategy\" if \"eval_strategy\" in sig_ta.parameters else \"evaluation_strategy\"\nta_args = {\n    \"output_dir\": OUT_DIR,\n    \"per_device_train_batch_size\": BSZ,\n    \"per_device_eval_batch_size\": BSZ,\n    \"gradient_accumulation_steps\": ACCUM,\n    \"num_train_epochs\": EPOCHS,\n    \"weight_decay\": WD,\n    eval_key: \"epoch\",\n    \"save_strategy\": \"no\",          # disable mid-run checkpoint writes\n    \"fp16\": torch.cuda.is_available(),\n    \"report_to\": \"none\",\n    \"logging_steps\": LOG_STEPS,\n    \"gradient_checkpointing\": True,\n    \"save_safetensors\": False,      # always use .bin\n    \"eval_accumulation_steps\": 32,\n    \"dataloader_num_workers\": 0,\n    \"warmup_ratio\": 0.1,\n    \"lr_scheduler_type\": \"cosine\",\n}\nta_args = {k: v for k, v in ta_args.items() if k in sig_ta.parameters}\nargs = TrainingArguments(**ta_args)\n\n# Layer-wise LR param groups\nno_decay = (\"bias\", \"LayerNorm.weight\", \"layer_norm.weight\")\nparam_groups = []\n\ndef add_group(named_params, lr):\n    decay, nodecay = [], []\n    for n, p in named_params:\n        if not p.requires_grad:\n            continue\n        if any(nd in n for nd in no_decay):\n            nodecay.append(p)\n        else:\n            decay.append(p)\n    if decay:\n        param_groups.append({\"params\": decay, \"weight_decay\": WD, \"lr\": lr})\n    if nodecay:\n        param_groups.append({\"params\": nodecay, \"weight_decay\": 0.0, \"lr\": lr})\n\nadd_group(model.classifier.named_parameters(), LR_HEAD)\nif hasattr(model.rembert, \"pooler\"):\n    add_group([(n, p) for n, p in model.rembert.pooler.named_parameters() if p.requires_grad], LR_LAST)\nif enc_layers is not None:\n    for blk in enc_layers[-N_TRAIN_LAYERS:]:\n        add_group(blk.named_parameters(), LR_LAST)\n\n# Weighted loss and custom optimizer\nclass WeightedTrainer(Trainer):\n    def create_optimizer(self):\n        if self.optimizer is None:\n            self.optimizer = AdamW(param_groups, lr=LR_LAST)\n        return self.optimizer\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.get(\"labels\")\n        outputs = model(**{k: v for k, v in inputs.items() if k != \"labels\"})\n        logits = outputs.get(\"logits\")\n        loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights.to(logits.device))\n        loss = loss_fn(logits.view(-1, num_labels), labels.view(-1))\n        return (loss, outputs) if return_outputs else loss\n\ntrainer = WeightedTrainer(\n    model=model,\n    args=args,\n    train_dataset=ds_tok[\"train\"],\n    eval_dataset=ds_tok[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\n# Early stopping\nclass EarlyStopper(TrainerCallback):\n    def __init__(self, metric=\"eval_f1\", patience=2, greater_is_better=True):\n        self.metric = metric\n        self.patience = patience\n        self.sign = 1.0 if greater_is_better else -1.0\n        self.best = -float(\"inf\") if greater_is_better else float(\"inf\")\n        self.bad = 0\n    def on_evaluate(self, args, state, control, **kw):\n        score = kw.get(\"metrics\", {}).get(self.metric)\n        if score is None:\n            return\n        if self.sign * score > self.sign * self.best:\n            self.best = score\n            self.bad = 0\n        else:\n            self.bad += 1\n            if self.bad >= self.patience:\n                print(f\"Early stopping on {self.metric}. Best={self.best:.4f}\")\n                control.should_training_stop = True\n\ntrainer.add_callback(EarlyStopper(metric=\"eval_f1\", patience=2, greater_is_better=True))\n\n# Train\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\ntrain_result = trainer.train()\ntrain_result.metrics\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T05:47:47.925736Z","iopub.execute_input":"2025-11-13T05:47:47.926382Z","iopub.status.idle":"2025-11-13T05:49:50.090677Z","shell.execute_reply.started":"2025-11-13T05:47:47.926358Z","shell.execute_reply":"2025-11-13T05:49:50.089852Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/686 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f90db17b2691452cae0eaddde0bbae4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.30G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b35bcfe0bf634f669655c577593a2e65"}},"metadata":{}},{"name":"stderr","text":"Some weights of RemBertForSequenceClassification were not initialized from the model checkpoint at google/rembert and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Trainable params: 49,151,234 / 575,922,690 (8.53%)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [125/125 01:48, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>No log</td>\n      <td>0.688197</td>\n      <td>0.520833</td>\n      <td>0.655396</td>\n      <td>0.520833</td>\n      <td>0.388441</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.691600</td>\n      <td>0.680461</td>\n      <td>0.611111</td>\n      <td>0.615477</td>\n      <td>0.611111</td>\n      <td>0.607400</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.681300</td>\n      <td>0.679754</td>\n      <td>0.618056</td>\n      <td>0.667076</td>\n      <td>0.618056</td>\n      <td>0.587822</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Early stopping on eval_f1. Best=0.6074\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'train_runtime': 109.5278,\n 'train_samples_per_second': 37.251,\n 'train_steps_per_second': 1.141,\n 'total_flos': 442219409530752.0,\n 'train_loss': 0.6843116455078125,\n 'epoch': 4.901960784313726}"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# %% [code]\nimport gc, torch\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T05:50:24.446334Z","iopub.execute_input":"2025-11-13T05:50:24.447099Z","iopub.status.idle":"2025-11-13T05:50:24.842492Z","shell.execute_reply.started":"2025-11-13T05:50:24.447070Z","shell.execute_reply":"2025-11-13T05:50:24.841672Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# %% [code]\n# 7) TRAIN — accuracy-focused fine-tuning (LLRD + class weights + early stop)\nimport os, gc, inspect\nimport torch\nfrom torch.optim import AdamW\nfrom transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom transformers.trainer_callback import TrainerCallback\n\n# ---- Memory/IO guards\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nOUT_DIR   = \"outputs_rembert\"\nBEST_DIR  = \"best_ckpt_rembert\"  # we'll save at end only\n\n# ---- Load model\nnum_labels = 2\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n\n# Disable KV cache during train\nif hasattr(model, \"config\") and hasattr(model.config, \"use_cache\"):\n    model.config.use_cache = False\n\n# Gradient checkpointing (version-proof)\ntry:\n    if hasattr(model, \"gradient_checkpointing_enable\"):\n        sig = inspect.signature(model.gradient_checkpointing_enable)\n        if \"use_reentrant\" in sig.parameters:\n            model.gradient_checkpointing_enable(use_reentrant=False)\n        else:\n            model.gradient_checkpointing_enable()\n    elif hasattr(model, \"enable_input_require_grads\"):\n        model.enable_input_require_grads()\n        if hasattr(model, \"config\"):\n            model.config.gradient_checkpointing = True\nexcept Exception:\n    if hasattr(model, \"config\"):\n        model.config.gradient_checkpointing = True\n\n# ---- Unfreeze strategy: last 3 encoder blocks + pooler + classifier\nFREEZE_ALL_BUT_LAST = True\nN_TRAIN_LAYERS = 3  # try 2 if VRAM tight; 3 usually fits with our tiny batch/len\nif FREEZE_ALL_BUT_LAST:\n    # Freeze all\n    for p in model.rembert.parameters():\n        p.requires_grad = False\n    # Unfreeze classifier\n    for p in model.classifier.parameters():\n        p.requires_grad = True\n    # Unfreeze pooler if present\n    if hasattr(model.rembert, \"pooler\"):\n        for p in model.rembert.pooler.parameters():\n            p.requires_grad = True\n    # Unfreeze last N layers\n    enc_layers = getattr(model.rembert.encoder, \"layer\", None)\n    if enc_layers is not None and N_TRAIN_LAYERS > 0:\n        for blk in enc_layers[-N_TRAIN_LAYERS:]:\n            for p in blk.parameters():\n                p.requires_grad = True\n\n    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    total     = sum(p.numel() for p in model.parameters())\n    print(f\"Trainable params: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)\")\n\n# ---- Hyperparams (very VRAM-friendly)\nEPOCHS    = 5           # allow more epochs; early stopping prevents overfit\nBSZ       = 1           # tiny per-device batch\nACCUM     = 24          # effective batch ~24\nLR_HEAD   = 3e-5        # higher LR for the head\nLR_LAST   = 2e-5        # last encoder blocks/pooler\nWD        = 0.01\nLOG_STEPS = 50\n\n# Prefer new eval key if supported\nTA = TrainingArguments\nsig_ta   = inspect.signature(TA.__init__)\neval_key = \"eval_strategy\" if \"eval_strategy\" in sig_ta.parameters else \"evaluation_strategy\"\n\ndesired_args = {\n    \"output_dir\": OUT_DIR,\n    \"learning_rate\": LR_LAST,           # base LR (we override with param groups below)\n    \"per_device_train_batch_size\": BSZ,\n    \"per_device_eval_batch_size\": BSZ,\n    \"gradient_accumulation_steps\": ACCUM,\n    \"num_train_epochs\": EPOCHS,\n    \"weight_decay\": WD,\n    eval_key: \"epoch\",\n    \"save_strategy\": \"no\",              # no mid-run checkpoint writes\n    \"fp16\": torch.cuda.is_available(),\n    \"report_to\": \"none\",\n    \"logging_steps\": LOG_STEPS,\n    \"gradient_checkpointing\": True,\n    \"save_safetensors\": False,          # avoid safetensors issue\n    \"eval_accumulation_steps\": 32,\n    \"dataloader_num_workers\": 0,\n    \"warmup_ratio\": 0.1,                # warmup 10%\n    \"lr_scheduler_type\": \"cosine\",      # cosine decay\n}\n\nsupported_args = {k: v for k, v in desired_args.items() if k in sig_ta.parameters}\ndropped = sorted(set(desired_args) - set(supported_args))\nif dropped:\n    print(\" Dropping unsupported TrainingArguments keys:\", dropped)\n\nargs = TA(**supported_args)\n\n# ---- Build Layer-wise LR param groups (LLRD)\nno_decay = (\"bias\", \"LayerNorm.weight\", \"layer_norm.weight\")\nparam_groups = []\n\ndef add_group(named_params, lr):\n    decay, nodecay = [], []\n    for n, p in named_params:\n        if not p.requires_grad:\n            continue\n        (nodecay if any(nd in n for nd in no_decay) else decay).append(p)\n    if decay:\n        param_groups.append({\"params\": decay, \"weight_decay\": WD, \"lr\": lr})\n    if nodecay:\n        param_groups.append({\"params\": nodecay, \"weight_decay\": 0.0, \"lr\": lr})\n\n# Head\nadd_group(model.classifier.named_parameters(), LR_HEAD)\n# Pooler (if trainable)\nif hasattr(model.rembert, \"pooler\"):\n    trainable_pool = [(n, p) for n,p in model.rembert.pooler.named_parameters() if p.requires_grad]\n    if trainable_pool:\n        add_group(trainable_pool, LR_LAST)\n# Last N blocks\nif hasattr(model.rembert, \"encoder\") and hasattr(model.rembert.encoder, \"layer\"):\n    layers = list(model.rembert.encoder.layer)\n    for i, blk in enumerate(layers[-N_TRAIN_LAYERS:], 1):\n        add_group(blk.named_parameters(), LR_LAST)\n\n# ---- Custom Trainer with class-weighted loss + our optimizer\nclass WeightedTrainer(Trainer):\n    def create_optimizer(self):\n        if self.optimizer is None:\n            self.optimizer = AdamW(param_groups, lr=LR_LAST)  # lr overridden by groups\n        return self.optimizer\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.get(\"labels\")\n        outputs = model(**{k:v for k,v in inputs.items() if k != \"labels\"})\n        logits = outputs.get(\"logits\")\n        # class_weights from cell 6a\n        loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights.to(logits.device))\n        loss = loss_fn(logits.view(-1, num_labels), labels.view(-1))\n        return (loss, outputs) if return_outputs else loss\n\ntrainer = WeightedTrainer(\n    model=model,\n    args=args,\n    train_dataset=ds_tok[\"train\"],\n    eval_dataset=ds_tok[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\n# ---- Early stopping (patience=2) — compatible fallback\nclass EarlyStopper(TrainerCallback):\n    def __init__(self, metric=\"eval_f1\", patience=2, greater_is_better=True):\n        self.metric = metric\n        self.patience = patience\n        self.sign = 1.0 if greater_is_better else -1.0\n        self.best = -float(\"inf\") if greater_is_better else float(\"inf\")\n        self.bad_epochs = 0\n\n    def on_evaluate(self, args, state, control, **kwargs):\n        metrics = kwargs.get(\"metrics\", {})\n        score = metrics.get(self.metric)\n        if score is None:\n            return\n        if self.sign * score > self.sign * self.best:\n            self.best = score\n            self.bad_epochs = 0\n        else:\n            self.bad_epochs += 1\n            if self.bad_epochs >= self.patience:\n                print(f\" Early stopping on {self.metric}. Best={self.best:.4f}\")\n                control.should_training_stop = True\n\ntrainer.add_callback(EarlyStopper(metric=\"eval_f1\", patience=2, greater_is_better=True))\n\n# ---- Free cache, then train\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\ntrain_result = trainer.train()\ntrain_result.metrics\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T05:50:26.700416Z","iopub.execute_input":"2025-11-13T05:50:26.701016Z","iopub.status.idle":"2025-11-13T05:54:06.679448Z","shell.execute_reply.started":"2025-11-13T05:50:26.700992Z","shell.execute_reply":"2025-11-13T05:54:06.678756Z"}},"outputs":[{"name":"stderr","text":"Some weights of RemBertForSequenceClassification were not initialized from the model checkpoint at google/rembert and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Trainable params: 49,151,234 / 575,922,690 (8.53%)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='170' max='170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [170/170 03:36, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.686428</td>\n      <td>0.541667</td>\n      <td>0.661194</td>\n      <td>0.541667</td>\n      <td>0.437367</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.689700</td>\n      <td>0.680303</td>\n      <td>0.555556</td>\n      <td>0.561277</td>\n      <td>0.555556</td>\n      <td>0.544934</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.682600</td>\n      <td>0.678272</td>\n      <td>0.631944</td>\n      <td>0.672771</td>\n      <td>0.631944</td>\n      <td>0.608836</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.682600</td>\n      <td>0.676272</td>\n      <td>0.611111</td>\n      <td>0.620401</td>\n      <td>0.611111</td>\n      <td>0.603462</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.675000</td>\n      <td>0.675601</td>\n      <td>0.590278</td>\n      <td>0.590295</td>\n      <td>0.590278</td>\n      <td>0.590258</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":" Early stopping on eval_f1. Best=0.6088\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"{'train_runtime': 217.6365,\n 'train_samples_per_second': 18.747,\n 'train_steps_per_second': 0.781,\n 'total_flos': 414304370709840.0,\n 'train_loss': 0.6814823936013614,\n 'epoch': 5.0}"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# %% [code]\nfrom pprint import pprint\n\n# Collect only evaluation logs (those that have eval_accuracy)\neval_logs = [\n    e for e in trainer.state.log_history\n    if isinstance(e, dict) and \"eval_accuracy\" in e\n]\n\nif eval_logs:\n    # Find the entry with the highest eval_accuracy\n    best = max(eval_logs, key=lambda x: x[\"eval_accuracy\"])\n\n    print(\"Best eval metrics across all epochs (by eval_accuracy):\")\n    pprint(best)\n\n    print(\"\\nSummary (best epoch):\")\n    print(f\"Epoch    : {best.get('epoch', 'N/A')}\")\n    print(f\"Accuracy : {best.get('eval_accuracy', 0):.4f}\")\n    print(f\"F1       : {best.get('eval_f1', 0):.4f}\")\n    print(f\"Precision: {best.get('eval_precision', 0):.4f}\")\n    print(f\"Recall   : {best.get('eval_recall', 0):.4f}\")\nelse:\n    print(\"No eval logs with 'eval_accuracy' found. Running a fresh evaluation...\\n\")\n    metrics = trainer.evaluate()\n    pprint(metrics)\n\n    print(\"\\nSummary (fresh eval):\")\n    print(f\"Accuracy : {metrics.get('eval_accuracy', 0):.4f}\")\n    print(f\"F1       : {metrics.get('eval_f1', 0):.4f}\")\n    print(f\"Precision: {metrics.get('eval_precision', 0):.4f}\")\n    print(f\"Recall   : {metrics.get('eval_recall', 0):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T05:57:11.139493Z","iopub.execute_input":"2025-11-13T05:57:11.140317Z","iopub.status.idle":"2025-11-13T05:57:11.146964Z","shell.execute_reply.started":"2025-11-13T05:57:11.140286Z","shell.execute_reply":"2025-11-13T05:57:11.146270Z"}},"outputs":[{"name":"stdout","text":"Best eval metrics across all epochs (by eval_accuracy):\n{'epoch': 3.0,\n 'eval_accuracy': 0.6319444444444444,\n 'eval_f1': 0.6088360412075241,\n 'eval_loss': 0.6782717108726501,\n 'eval_precision': 0.6727709017428644,\n 'eval_recall': 0.6319444444444444,\n 'eval_runtime': 4.9851,\n 'eval_samples_per_second': 28.886,\n 'eval_steps_per_second': 28.886,\n 'step': 102}\n\nSummary (best epoch):\nEpoch    : 3.0\nAccuracy : 0.6319\nF1       : 0.6088\nPrecision: 0.6728\nRecall   : 0.6319\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# %% [code]\nimport os, torch\nSAVE_DIR = \"model_validity_rembert_ft\"\nos.makedirs(SAVE_DIR, exist_ok=True)\nm = trainer.model\n\ntry:\n    m.save_pretrained(SAVE_DIR, safe_serialization=False, max_shard_size=\"500MB\")\nexcept TypeError:\n    torch.save(m.state_dict(), os.path.join(SAVE_DIR, \"pytorch_model.bin\"))\n\ntokenizer.save_pretrained(SAVE_DIR)\nprint(\"Saved to\", SAVE_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T05:54:22.374402Z","iopub.execute_input":"2025-11-13T05:54:22.374648Z","iopub.status.idle":"2025-11-13T05:54:25.668220Z","shell.execute_reply.started":"2025-11-13T05:54:22.374619Z","shell.execute_reply":"2025-11-13T05:54:25.667438Z"}},"outputs":[{"name":"stdout","text":"Saved to model_validity_rembert_ft\n","output_type":"stream"}],"execution_count":11}]}